{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icu_experiments.load_data import load_data_for_prediction\n",
    "from icu_experiments.preprocessing import make_feature_preprocessing, make_anchor_preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor, Booster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from ivmodels import AnchorRegression\n",
    "\n",
    "outcome = \"hr\"\n",
    "\n",
    "def load_data(name, outcome):\n",
    "    Xy = load_data_for_prediction([name], outcome=outcome, log_transform=True)\n",
    "    Xy_test = Xy[name]['train']\n",
    "    Xy_tuning_data = Xy[name]['test']\n",
    "    return Xy_test, Xy_tuning_data\n",
    "\n",
    "Xy_train, Xy_test = load_data('eicu', outcome)\n",
    "Xy_test_hirid, Xy_tuning_hirid = load_data('hirid', outcome)\n",
    "Xy_test_mimic, Xy_tuning_mimic = load_data('mimic', outcome) \n",
    "Xy_test_miiv, Xy_tuning_miiv = load_data('miiv', outcome) \n",
    "\n",
    "preprocessing_steps = make_feature_preprocessing(missing_indicator=True)\n",
    "preprocessor = ColumnTransformer(transformers=preprocessing_steps).set_output(transform=\"pandas\") # Allow to preprocess subbsets of data differently\n",
    "\n",
    "anchor_columns = ['hospital_id']\n",
    "anchor_preprocessing_steps = make_anchor_preprocessing(anchor_columns)\n",
    "anchor_preprocessor = ColumnTransformer(\n",
    "        anchor_preprocessing_steps + preprocessing_steps #preprocessing_steps\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "p1 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "p2 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "p3 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "p4 = Pipeline(steps=[\n",
    "    ('preprocessing', anchor_preprocessor),\n",
    "    ('model', AnchorRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance from Training to Target Data - Parameters chosen via GridCV on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'model__boosting_type': ['gbdt'],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'model__n_estimators': [100, 800],\n",
    "    'model__num_leaves': [50, 200, 1024],\n",
    "    'model__feature_fraction': [0.5, 0.9]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'model__boosting_type': ['rf'],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'model__n_estimators': [100, 800],\n",
    "    'model__num_leaves': [50, 200, 1024],\n",
    "    'model__feature_fraction': [0.5, 0.9]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(p1, param_grid=param_grid_lgbm)\n",
    "search.fit(Xy_train, Xy_train['outcome'])\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "grid_search_params_lgbm = search.best_params_\n",
    "print(search.best_params_)\n",
    "\n",
    "search = GridSearchCV(p2, param_grid=param_grid_rf)\n",
    "search.fit(Xy_train, Xy_train['outcome'])\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "grid_search_params_rf = search.best_params_\n",
    "print(search.best_params_)\n",
    "\n",
    "p1.set_params(**grid_search_params_lgbm)\n",
    "p1.fit(Xy_train, Xy_train['outcome'])\n",
    "p2.set_params(**grid_search_params_rf)\n",
    "p2.fit(Xy_train, Xy_train['outcome'])\n",
    "\n",
    "mse_grid_lgbm_hirid = mean_squared_error(Xy_test_hirid['outcome'], p1.predict(Xy_test_hirid))\n",
    "mse_grid_rf_hirid = mean_squared_error(Xy_test_hirid['outcome'], p2.predict(Xy_test_hirid))\n",
    "\n",
    "mse_grid_lgbm_mimic = mean_squared_error(Xy_test_mimic['outcome'], p1.predict(Xy_test_mimic))\n",
    "mse_grid_rf_mimic = mean_squared_error(Xy_test_mimic['outcome'], p2.predict(Xy_test_mimic))\n",
    "\n",
    "mse_grid_lgbm_miiv = mean_squared_error(Xy_test_miiv['outcome'], p1.predict(Xy_test_miiv))\n",
    "mse_grid_rf_miiv = mean_squared_error(Xy_test_miiv['outcome'], p2.predict(Xy_test_miiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance from Training to Target Data - Parameters chosen via Evaluation on Target Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: \n",
    "- Train Data with different parameters on Training set\n",
    "- Evaluate Train Data on fine tuning data from target set \n",
    "- choose the best performing parameters\n",
    "- do this for all possible n from the fine tuning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 200, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'boosting_type': ['rf'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 800], \n",
    "    'num_leaves': [50, 200, 1024], \n",
    "    'feature_fraction': [0.5, 0.9]\n",
    "}\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10, 10000],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.00001, 0.001, 0.1]\n",
    "}\n",
    "\n",
    "def find_best_parameters(Xy_train, Xy_tuning_data, p, param_grid):\n",
    "    results_for_n = []\n",
    "\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        best_params = None\n",
    "        best_mse = float('inf')  # Initialize with a large value\n",
    "\n",
    "        # Iterate over all possible combinations of hyperparameters\n",
    "        for param_set in itertools.product(*param_grid.values()):\n",
    "            params = dict(zip(param_grid.keys(), param_set))\n",
    "        \n",
    "            p.named_steps['model'].set_params(**params)\n",
    "            p.fit(Xy_train, Xy_train['outcome'])\n",
    "            y_pred = p.predict(Xy_tuning_data.head(n))\n",
    "            mse = mean_squared_error(Xy_tuning_data['outcome'].head(n), y_pred)\n",
    "\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_params = params\n",
    "\n",
    "        results_for_n.append({'n': n, 'best_params': best_params, 'best_mse': best_mse})\n",
    "\n",
    "    return results_for_n\n",
    "\n",
    "results_p1_hirid = find_best_parameters(Xy_train, Xy_tuning_hirid, p1, param_grid_lgbm)\n",
    "results_p2_hirid = find_best_parameters(Xy_train, Xy_tuning_hirid, p2, param_grid_rf)\n",
    "results_p4_hirid = find_best_parameters(Xy_train, Xy_tuning_hirid, p4, param_grid_anchor)\n",
    "\n",
    "results_p1_mimic = find_best_parameters(Xy_train, Xy_tuning_mimic, p1, param_grid_lgbm)\n",
    "results_p2_mimic = find_best_parameters(Xy_train, Xy_tuning_mimic, p2, param_grid_rf)\n",
    "results_p4_mimic = find_best_parameters(Xy_train, Xy_tuning_mimic, p4, param_grid_anchor)\n",
    "\n",
    "results_p1_miiv = find_best_parameters(Xy_train, Xy_tuning_miiv, p1, param_grid_lgbm)\n",
    "results_p2_miiv = find_best_parameters(Xy_train, Xy_tuning_miiv, p2, param_grid_rf)\n",
    "results_p4_miiv = find_best_parameters(Xy_train, Xy_tuning_miiv, p4, param_grid_anchor)\n",
    "\n",
    "def calculate_mse(X_train, y_train, X_test, y_test, p, results):\n",
    "    mse_for_n = []\n",
    "    i = 0\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        p.named_steps['model'].set_params(**results[i]['best_params'])\n",
    "        p.fit(X_train, y_train)\n",
    "        y_pred = p.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_for_n.append({'n': n, 'mse': mse})\n",
    "        i += 1\n",
    "    return mse_for_n\n",
    "\n",
    "mse_eicu_to_hirid_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p1, results_p1_hirid)\n",
    "mse_eicu_to_hirid_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p2, results_p2_hirid)\n",
    "mse_eicu_to_hirid_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p4, results_p4_hirid)\n",
    "\n",
    "mse_eicu_to_mimic_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p1, results_p1_mimic)\n",
    "mse_eicu_to_mimic_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p2, results_p2_mimic)\n",
    "mse_eicu_to_mimic_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p4, results_p4_mimic)\n",
    "\n",
    "mse_eicu_to_miiv_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_miiv, Xy_test_miiv['outcome'], p1, results_p1_miiv)\n",
    "mse_eicu_to_miiv_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_miiv, Xy_test_miiv['outcome'], p2, results_p2_miiv)\n",
    "mse_eicu_to_miiv_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_miiv, Xy_test_miiv['outcome'], p4, results_p4_miiv)\n",
    "\n",
    "# OLS MSE Calculation\n",
    "p3.fit(Xy_train, Xy_train['outcome'])\n",
    "mse_eicu_to_hirid_p3 = mean_squared_error(Xy_test_hirid['outcome'], p3.predict(Xy_test_hirid))\n",
    "mse_eicu_to_hirid_dummy_prediction = mean_squared_error(Xy_test_hirid['outcome'], np.full_like(Xy_test_hirid['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "mse_eicu_to_mimic_p3 = mean_squared_error(Xy_test_mimic['outcome'], p3.predict(Xy_test_mimic))\n",
    "mse_eicu_to_mimic_dummy_prediction = mean_squared_error(Xy_test_mimic['outcome'], np.full_like(Xy_test_mimic['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "mse_eicu_to_miiv_p3 = mean_squared_error(Xy_test_miiv['outcome'], p3.predict(Xy_test_miiv))\n",
    "mse_eicu_to_miiv_dummy_prediction = mean_squared_error(Xy_test_miiv['outcome'], np.full_like(Xy_test_miiv['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "def plotting(mse_p1, mse_p2, mse_p3, mse_p4, mse_baseline1, mse_baseline2, train, target):\n",
    "    n = [25, 50, 100, 200, 400, 800, 1600]\n",
    "    plt.plot(n, ([item['mse'] for item in mse_p1]), marker='o', linestyle='-', label = 'LGBM')\n",
    "    plt.plot(n, [item['mse'] for item in mse_p2], marker='o', linestyle='-', label = 'RF')\n",
    "    plt.plot(n, [item['mse'] for item in mse_p4], marker='o', linestyle='-', label = 'Anchor')\n",
    "    plt.axhline(y=mse_p3, color='black', linestyle='-', label='OLS Baseline')\n",
    "    plt.axhline(y=mse_baseline1, color='green', linestyle='-', label='LGBM Baseline')\n",
    "    plt.axhline(y=mse_baseline2, color='purple', linestyle='-', label='RF Baseline')\n",
    "    #plt.axhline(y = mean_squared_error(Xy_test_new['outcome'], np.full_like(Xy_test_new['outcome'],Xy_train[outcome].mean())), color = 'black', label='Train Average')\n",
    "    plt.title(f'Parameters for Model chosen with evaluation on n Data Points from Target Distribution {target}')\n",
    "    plt.xlabel('Number of Data Points (n)')\n",
    "    plt.ylabel('Mean Squared Error (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plotting(mse_eicu_to_hirid_p1, mse_eicu_to_hirid_p2, mse_eicu_to_hirid_p3, mse_eicu_to_hirid_p4, mse_grid_lgbm_hirid, mse_grid_rf_hirid, 'Eicu', 'Hirid')\n",
    "plotting(mse_eicu_to_mimic_p1, mse_eicu_to_mimic_p2, mse_eicu_to_mimic_p3, mse_eicu_to_mimic_p4, mse_grid_lgbm_mimic, mse_grid_rf_mimic, 'Eicu', 'Mimic')\n",
    "plotting(mse_eicu_to_miiv_p1, mse_eicu_to_miiv_p2, mse_eicu_to_miiv_p3, mse_eicu_to_miiv_p4, mse_grid_lgbm_miiv, mse_grid_rf_miiv, 'Eicu', 'Miiv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Woche5/parameters_dict.pkl\n",
      "MSE values saved to Woche5/mse_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "mse_dict = {}\n",
    "\n",
    "results_dict['p1_baseline'] = grid_search_params_lgbm\n",
    "results_dict['p2_baseline'] = grid_search_params_rf\n",
    "results_dict['p1_hirid'] = results_p1_hirid\n",
    "results_dict['p2_hirid'] = results_p2_hirid\n",
    "results_dict['p4_hirid'] = results_p4_hirid\n",
    "\n",
    "results_dict['p1_mimic'] = results_p1_mimic\n",
    "results_dict['p2_mimic'] = results_p2_mimic\n",
    "results_dict['p4_mimic'] = results_p4_mimic\n",
    "\n",
    "results_dict['p1_miiv'] = results_p1_miiv\n",
    "results_dict['p2_miiv'] = results_p2_miiv\n",
    "results_dict['p4_miiv'] = results_p4_miiv\n",
    "\n",
    "mse_dict['eicu_to_hirid_p1_baseline'] = mse_grid_lgbm_hirid\n",
    "mse_dict['eicu_to_hirid_p1'] = mse_eicu_to_hirid_p1\n",
    "mse_dict['eicu_to_hirid_p2'] = mse_eicu_to_hirid_p2\n",
    "mse_dict['eicu_to_hirid_p3'] = mse_eicu_to_hirid_p3\n",
    "mse_dict['eicu_to_hirid_p4'] = mse_eicu_to_hirid_p4\n",
    "\n",
    "mse_dict['eicu_to_mimic_p1_baseline'] = mse_grid_lgbm_mimic\n",
    "mse_dict['eicu_to_mimic_p1'] = mse_eicu_to_mimic_p1\n",
    "mse_dict['eicu_to_mimic_p2'] = mse_eicu_to_mimic_p2\n",
    "mse_dict['eicu_to_mimic_p3'] = mse_eicu_to_mimic_p3\n",
    "mse_dict['eicu_to_mimic_p4'] = mse_eicu_to_mimic_p4\n",
    "\n",
    "mse_dict['eicu_to_miiv_p1_baseline'] = mse_grid_lgbm_miiv\n",
    "mse_dict['eicu_to_miiv_p1'] = mse_eicu_to_miiv_p1\n",
    "mse_dict['eicu_to_miiv_p2'] = mse_eicu_to_miiv_p2\n",
    "mse_dict['eicu_to_miiv_p3'] = mse_eicu_to_miiv_p3\n",
    "mse_dict['eicu_to_miiv_p4'] = mse_eicu_to_miiv_p4\n",
    "\n",
    "mse_dict['eicu_to_hirid_dummy_prediction'] = mse_eicu_to_hirid_dummy_prediction\n",
    "mse_dict['eicu_to_mimic_dummy_prediction'] = mse_eicu_to_mimic_dummy_prediction\n",
    "mse_dict['eicu_to_miiv_dummy_prediction'] = mse_eicu_to_miiv_dummy_prediction\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the file paths to save the dictionaries\n",
    "results_file_path = 'Woche5/parameters_dict.pkl'\n",
    "mse_file_path = 'Woche5/mse_dict.pkl'\n",
    "\n",
    "# Save the results dictionary to a file\n",
    "with open(results_file_path, 'wb') as results_file:\n",
    "    pickle.dump(results_dict, results_file)\n",
    "\n",
    "# Save the MSE dictionary to a file\n",
    "with open(mse_file_path, 'wb') as mse_file:\n",
    "    pickle.dump(mse_dict, mse_file)\n",
    "\n",
    "# Optionally, you can print a message to confirm the saving process\n",
    "print(f\"Results saved to {results_file_path}\")\n",
    "print(f\"MSE values saved to {mse_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedAnchor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, anchor_params=None, lgbm_params=None):\n",
    "        # Initialize parameters\n",
    "        self.anchor_params = anchor_params if anchor_params is not None else {}\n",
    "        self.lgbm_params = lgbm_params if lgbm_params is not None else {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize and fit the Anchor Regression model\n",
    "        self.anchor_model = AnchorRegression(**self.anchor_params)\n",
    "        self.anchor_model.fit(X, y)\n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = y - self.anchor_model.predict(X)\n",
    "\n",
    "        # Initialize and fit the LGBMRegressor with residuals\n",
    "        self.lgbm_model = LGBMRegressor(**self.lgbm_params)\n",
    "        self.lgbm_model.fit(X, residuals)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if fit has been called\n",
    "        if not hasattr(self, 'anchor_model') or not hasattr(self, 'lgbm_model'):\n",
    "            raise AttributeError(\"Models have not been fitted. Call fit() first.\")\n",
    "\n",
    "        # Make predictions\n",
    "        anchor_predictions = self.anchor_model.predict(X)\n",
    "        lgbm_predictions = self.lgbm_model.predict(X)\n",
    "\n",
    "        # Combine predictions\n",
    "        return anchor_predictions + lgbm_predictions\n",
    "    \n",
    "p5 = Pipeline(steps=[\n",
    "    ('preprocessing', anchor_preprocessor),\n",
    "    ('model', CustomizedAnchor())\n",
    "])\n",
    "\n",
    "\n",
    "def find_custom_parameters(Xy_train, Xy_tuning_data, p, param1_grid, param2_grid):\n",
    "    results_for_n = []\n",
    "\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        best_params1 = None\n",
    "        best_params2 = None\n",
    "        best_mse = float('inf') \n",
    "\n",
    "        param1_combinations = list(itertools.product(*param1_grid.values()))\n",
    "        param1_combinations_bar = tqdm(param1_combinations, desc=f\"n = {n}\")\n",
    "\n",
    "        for param1_set in itertools.product(*param1_grid.values()):\n",
    "            param1 = dict(zip(param1_grid.keys(), param1_set))\n",
    "\n",
    "            for param2_set in itertools.product(*param2_grid.values()):\n",
    "                param2 = dict(zip(param2_grid.keys(), param2_set))\n",
    "                \n",
    "\n",
    "                p.named_steps['model'].set_params(anchor_params= param1, lgbm_params=param2)\n",
    "                p.fit(Xy_train, Xy_train['outcome'])\n",
    "                y_pred = p.predict(Xy_tuning_data.head(n))\n",
    "                mse = mean_squared_error(Xy_tuning_data['outcome'].head(n), y_pred)\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_params1 = param1\n",
    "                    best_params2 = param2\n",
    "\n",
    "        results_for_n.append({'n': n, 'best_params set 1': best_params1, 'best_params set 2': best_params2, 'best_mse': best_mse})\n",
    "\n",
    "    return results_for_n\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10, 10000],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.00001, 0.001, 0.1]\n",
    "}\n",
    "\n",
    "results_p5_hirid = find_custom_parameters(Xy_train, Xy_tuning_hirid, p5, param_grid_anchor, param_grid_lgbm)\n",
    "#results_p5_mimic = find_custom_parameters(Xy_train, Xy_tuning_mimic, p5, param_grid_anchor, param_grid_lgbm)\n",
    "#results_p5_miiv = find_custom_parameters(Xy_train, Xy_tuning_miiv, p5, param_grid_anchor, param_grid_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(X_train, y_train, X_test, y_test, p, results):\n",
    "    mse_for_n = []\n",
    "    i = 0\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        p.named_steps['model'].set_params(anchor_params= results[i]['best_params set 1'], lgbm_params= results[i]['best_params set 2'])\n",
    "        p.fit(X_train, y_train)\n",
    "        y_pred = p.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_for_n.append({'n': n, 'mse': mse})\n",
    "        i += 1\n",
    "    return mse_for_n\n",
    "\n",
    "mse_eicu_to_hirid_p5 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p5, results_p5_hirid)\n",
    "#mse_eicu_to_mimic_p5 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p5, results_p5_mimic)\n",
    "#mse_eicu_to_miiv_p5 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p5, results_p5_miiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations eICU to X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "The hyperparameters were selected to minimize the Mean Squared Error (MSE) on the fine-tuning dataset of the target distribution. This fine-tuning dataset consists of various sizes, including n = 25, 50, 100, 200, 400, 800, and 1600 data points from the target distribution. Initially, we randomly selected 1600 data points from the target data and named it Xy_tuning_data, which is distinct from the final evaluation dataset used to generate the plotted MSE after model training, called Xy_test_new. \n",
    "```\n",
    "**Evaluation Process:**\n",
    "```markdown\n",
    "Our evaluation process follows these steps:\n",
    "\n",
    "1. For each combination of the parameters, we train the model on the training data.\n",
    "2. Next, we calculate the MSE on the fine-tuning data from the training distribution.\n",
    "3. For each n value, we select the parameter combination that minimizes the MSE on the fine-tuning data.\n",
    "\n",
    "We have four distinct pipelines for our models:\n",
    "\n",
    "- LGBM pipeline: p1\n",
    "- Random Forest pipeline: p2\n",
    "- OLS pipeline: p3\n",
    "- Anchor pipeline: p4\n",
    "\n",
    "For OLS, we follow a slightly different approach. We train the model on the training data and evaluate it directly on the target data.\n",
    "\n",
    "In a subsequent step, we repeat the parameter selection process on the training data and calculate the MSE on the target data - we call this approach the Baseline. The plot displays the model's performance along with the Baseline.\n",
    "```\n",
    "\n",
    "**Model Performance:**\n",
    "```markdown\n",
    "Interestingly, none of the models managed to substentially outperform the Baselines on any dataset. \n",
    "\n",
    "eICU --> Hirid:\n",
    "- p1/LGBM: \n",
    "    - In an overall trend, LGBM is able improve its precision with increasing n, however, it is still not able to get the same precision as the Baseline\n",
    "    - Similar to the RF, it restricts the number of leafs drastically\n",
    "- p2/RF: \n",
    "    - The same parameters have been chosen every time\n",
    "    - Its parameters coincide with those choosen by GridCV for n < 1600 \n",
    "    - Its performance decreses when allowing 1600 fine-tuning datapoints, aka as soon as the distr. shift becomes noticable\n",
    "    - Interestengly, it choses a small number of leaves compared to the size of the available fine-tuning dataset\n",
    "- p4/Anchor and p3/OLS:\n",
    "    - Surprisingly, Anchor fails to identify a significant distributional shift, aka. it chooses consistently gamma = 1, i.e. it coincides with OLS\n",
    "    - Unsurprisingly, it performs almost identical to OLS, only the regularization influences the performance\n",
    "    - The more fine-tuning data we allow, the less regularization it choses\n",
    "    - Not able to beat the LGBM Baseline\n",
    "\n",
    "eICU --> Mimic:\n",
    "- p1/LGBM: \n",
    "    - In an overall trend, LGBM is able improve its precision with increasing n, and it is able to consistently beat its Baseline\n",
    "    - It is able to improve its performance significantly by restricting itself to a small number of leafs\n",
    "- p2/RF: \n",
    "    - The same parameters have been chosen for every n \n",
    "    - Its parameters coincide with those choosen by GridCV and the performance too\n",
    "    - It does not seem to notice a distr. shift\n",
    "    - Interestengly, it choses a small number of leaves compared to the size of the available fine-tuning dataset\n",
    "- p4/Anchor and p3/OLS:\n",
    "    - Surprisingly, Anchor fails to identify a significant distributional shift, aka. it chooses consistently gamma = 1, i.e. it coincides with OLS\n",
    "    - Best performing model \n",
    "\n",
    "eICU --> Miiv:\n",
    "- p1/LGBM: \n",
    "    - In an overall trend, LGBM is able improve its precision with increasing n, but not able to consistently beat its Baseline\n",
    "    - It is able to improve its performance significantly by restricting itself to a small number of leafs\n",
    "- p2/RF: \n",
    "    - It is able to adapt itself to the baseline parameters and coincides most of the time with the Baseline\n",
    "    - It does not seem to notice a distr. shift\n",
    "    - Interestengly, it choses a small number of leaves compared to the size of the available fine-tuning dataset\n",
    "- p4/Anchor and p3/OLS:\n",
    "    - Surprisingly, Anchor a distributional shift in the beginning, but fails to identify it consistently\n",
    "    - However, Anchor is able to outperform OLS by a margin, most likely due to the regularization / OLS seems to have highly correlated features that destroy its prediction\n",
    "    - Not able to beat the LGBM Baseline\n",
    "\n",
    "The evaluation mse on the fine-tuning data when performing parameter selection does not seem to have any predictive power of the outcome of the mse on the target data\n",
    "\n",
    "This observation could be attributed to the limited available hyperparameters. It would be intriguing to investigate whether the models can surpass their Baseline when provided with more possibilities. A potential follow-up question is whether predictive performance improves with n=2000 (Hypothesis: Yes, as the prediction benefits from more accurate data).\n",
    "\n",
    "Remarkably, all models outperformed the average prediction of the training data by a substantial margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "The hyperparameters were chosen from three distinct parameter grids:\n",
    "```\n",
    "\n",
    "**LightGBM (param_grid_lgbm):**\n",
    "```python\n",
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 200, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "```\n",
    "\n",
    "**RF (param_grid_rf):**\n",
    "```python\n",
    "param_grid_rf = {\n",
    "    'boosting_type': ['rf'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 800], \n",
    "    'num_leaves': [50, 200, 1024], \n",
    "    'feature_fraction': [0.5, 0.9]\n",
    "}\n",
    "```\n",
    "\n",
    "**Anchor (param_grid_anchor):**\n",
    "```python\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10, 10000],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.00001, 0.001, 0.1]\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom Anchor:**\n",
    "```python\n",
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.001, 0.1]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for both, LGBM and RF the algorithm chooses the same set of parameters when evaluated on the tuning set from the target date every time. Hence unsurprisingly, the mse is constant. Similar for Anchor, we choose alomst every time the same set of parameters and do not improve the mse.\n",
    "\n",
    "For LGBM: The set of parameters chosen by grid search outperforms the parameters chosen by evaluation on the target. \n",
    "\n",
    "For RF: The set of parameters is the same, i.e. same performance \n",
    "\n",
    "For Anchor: No CV on train\n",
    "\n",
    "However, Anchor is again able to beat the predictive performance from OLS with the available parameters. \n",
    "\n",
    "For CustomAnchor (Anchor + LGBM Boosting): The set of parameters improves when increasing the evaluation data from the target. This method outperforms all other methods when having 1600 fine-tuning data points available.\n",
    "\n",
    "We conclude by noting that it is of utmost importance to include hyperparameters that prevent overfitting of the tree methods (compare results from parameterset 1 and parameterset 2) and are curious if the performance of CustomAnchor can be improved too when allowing these kind of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Mimic without children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test_mimic_no_children, Xy_tuning_mimic_no_children = Xy_test_mimic[Xy_test_mimic['age'] > 18], Xy_tuning_mimic[Xy_tuning_mimic['age'] > 18]\n",
    "\n",
    "mse_grid_lgbm_mimic_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], p1.predict(Xy_test_mimic_no_children))\n",
    "mse_grid_rf_mimic_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], p2.predict(Xy_test_mimic_no_children))\n",
    "\n",
    "results_p1_mimic_nC = find_best_parameters(Xy_train, Xy_tuning_mimic_no_children, p1, param_grid_lgbm)\n",
    "results_p2_mimic_nC = find_best_parameters(Xy_train, Xy_tuning_mimic_no_children, p2, param_grid_rf)\n",
    "results_p4_mimic_nC = find_best_parameters(Xy_train, Xy_tuning_mimic_no_children, p4, param_grid_anchor)\n",
    "\n",
    "mse_eicu_to_mimic_p1_nC = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic_no_children, Xy_test_mimic_no_children['outcome'], p1, results_p1_mimic_nC)\n",
    "mse_eicu_to_mimic_p2_nC = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic_no_children, Xy_test_mimic_no_children['outcome'], p2, results_p2_mimic_nC)\n",
    "mse_eicu_to_mimic_p4_nC = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic_no_children, Xy_test_mimic_no_children['outcome'], p4, results_p4_mimic_nC)\n",
    "\n",
    "mse_eicu_to_mimic_p3_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], p3.predict(Xy_test_mimic_no_children))\n",
    "mse_eicu_to_mimic_dummy_prediction_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], np.full_like(Xy_test_mimic_no_children['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "plotting(mse_eicu_to_mimic_p1_nC, mse_eicu_to_mimic_p2_nC, mse_eicu_to_mimic_p3_nC, mse_eicu_to_mimic_p4_nC, mse_grid_lgbm_mimic_nC, mse_grid_rf_mimic_nC, 'Eicu', 'Mimic no Children')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Malte Fragen beantworten: \n",
    "    - peak: Hab mit falschen Parametern getestet\n",
    "    - Sowohl RF wie LGBM mit tuning auf target distr. data sind schlechter (nie besser) als die Baselines, egal wie gross “n” ist. Wieso?\n",
    "    - Dein MSE der OLS baseline eICU -> MIMIC III ist signifikant besser als das was ich in dem pdf das ich dir mal geschickt hatte habe (~175). Was machst du anders? ################## das ist eICU -> Hirid\n",
    "- Refit implementieren und anschauen\n",
    "- Euler\n",
    "- Connect to ADA and run jobs - muss ich dann icu & iv neu installieren --- wie?\n",
    "- ML Flow lernen "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
