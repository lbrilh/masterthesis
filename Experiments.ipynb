{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icu_experiments.load_data import load_data_for_prediction\n",
    "from icu_experiments.preprocessing import make_feature_preprocessing, make_anchor_preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor, Booster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from ivmodels import AnchorRegression\n",
    "from plotting import plot_tuning\n",
    "\n",
    "outcome = \"hr\"\n",
    "\n",
    "sources = ['eicu', 'hirid', 'mimic', 'miiv']\n",
    "regressors = ['lgbm', 'rf', 'ols', 'anchor']\n",
    "_data = load_data_for_prediction(sources,  outcome=outcome, log_transform=True)\n",
    "\n",
    "preprocessor_ols = ColumnTransformer(transformers=make_feature_preprocessing(missing_indicator=True)).set_output(transform=\"pandas\") # Allow to preprocess subbsets of data differently\n",
    "\n",
    "\n",
    "#### LGBM Categorical\n",
    "preprocessor_lgbm = ColumnTransformer(transformers=make_feature_preprocessing(missing_indicator=False)).set_output(transform=\"pandas\") # Allow to preprocess subbsets of data differently\n",
    "\n",
    "\n",
    "anchor_columns = ['hospital_id']\n",
    "anchor_preprocessor = ColumnTransformer(\n",
    "        make_anchor_preprocessing(anchor_columns) + make_feature_preprocessing(missing_indicator=True) #preprocessing_steps\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "pipeline_lgbm = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor_lgbm),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor_lgbm),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "pipeline_ols = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor_ols),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline_anchor = Pipeline(steps=[\n",
    "    ('preprocessing', anchor_preprocessor),\n",
    "    ('model', AnchorRegression())\n",
    "])\n",
    "\n",
    "pipelines = {'lgbm': pipeline_lgbm,\n",
    "             'rf': pipeline_rf,\n",
    "             'ols': pipeline_ols,\n",
    "             'anchor': pipeline_anchor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['lgbm'] = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 800],\n",
    "    'num_leaves': [50, 200, 1024],\n",
    "    'feature_fraction': [0.5, 0.9],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "params['rf'] = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 800],\n",
    "    'num_leaves': [50, 200, 1024],\n",
    "    'feature_fraction': [0.5, 0.9],\n",
    "    'verbose': [-1]\n",
    "}\n",
    "params['anchor'] = {\n",
    "    'gamma': [1, 10, 10000],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.00001, 0.001, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': ['gbdt'],\n",
       " 'learning_rate': [0.01, 0.1, 0.3],\n",
       " 'n_estimators': [100, 800],\n",
       " 'num_leaves': [50, 200, 1024],\n",
       " 'feature_fraction': [0.5, 0.9],\n",
       " 'verbose': [0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['lgbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>cai</th>\n",
       "      <th>ptt</th>\n",
       "      <th>ast</th>\n",
       "      <th>be</th>\n",
       "      <th>bili</th>\n",
       "      <th>urine</th>\n",
       "      <th>mg</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>urgency</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>icu_adm_dow</th>\n",
       "      <th>hosp_adm_dow</th>\n",
       "      <th>adm_caregiver</th>\n",
       "      <th>adm_provider</th>\n",
       "      <th>numbedscategory</th>\n",
       "      <th>teachingstatus</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>hirid</td>\n",
       "      <td>15177</td>\n",
       "      <td>0.141433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.999626</td>\n",
       "      <td>6.716667</td>\n",
       "      <td>-1.047533</td>\n",
       "      <td>5.623972</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>hirid</td>\n",
       "      <td>18350</td>\n",
       "      <td>0.133510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>-1.466667</td>\n",
       "      <td>-1.229855</td>\n",
       "      <td>5.734066</td>\n",
       "      <td>0.556503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5796</th>\n",
       "      <td>hirid</td>\n",
       "      <td>23413</td>\n",
       "      <td>0.123886</td>\n",
       "      <td>3.543854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.540000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.560046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>hirid</td>\n",
       "      <td>13514</td>\n",
       "      <td>0.131028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.808582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>hirid</td>\n",
       "      <td>24392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.306136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>hirid</td>\n",
       "      <td>17659</td>\n",
       "      <td>0.133817</td>\n",
       "      <td>4.264840</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>-3.537500</td>\n",
       "      <td>0.492912</td>\n",
       "      <td>5.624400</td>\n",
       "      <td>0.377477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7891</th>\n",
       "      <td>hirid</td>\n",
       "      <td>31946</td>\n",
       "      <td>0.143076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.620000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.917566</td>\n",
       "      <td>0.737480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>hirid</td>\n",
       "      <td>19693</td>\n",
       "      <td>0.256127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>-2.020000</td>\n",
       "      <td>-1.452998</td>\n",
       "      <td>4.216416</td>\n",
       "      <td>0.760469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>hirid</td>\n",
       "      <td>13171</td>\n",
       "      <td>0.130990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.583333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.084565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>hirid</td>\n",
       "      <td>11135</td>\n",
       "      <td>0.112975</td>\n",
       "      <td>3.485816</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.345411</td>\n",
       "      <td>0.613866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Friday</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[missing]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6781 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     source  stay_id       cai       ptt       ast        be      bili  \\\n",
       "3774  hirid    15177  0.141433       NaN  4.999626  6.716667 -1.047533   \n",
       "4551  hirid    18350  0.133510       NaN  2.833213 -1.466667 -1.229855   \n",
       "5796  hirid    23413  0.123886  3.543854       NaN -2.540000       NaN   \n",
       "3345  hirid    13514  0.131028       NaN       NaN  0.300000       NaN   \n",
       "6057  hirid    24392       NaN       NaN       NaN       NaN       NaN   \n",
       "...     ...      ...       ...       ...       ...       ...       ...   \n",
       "4373  hirid    17659  0.133817  4.264840  5.017280 -3.537500  0.492912   \n",
       "7891  hirid    31946  0.143076       NaN       NaN -6.620000       NaN   \n",
       "4859  hirid    19693  0.256127       NaN  2.564949 -2.020000 -1.452998   \n",
       "3264  hirid    13171  0.130990       NaN       NaN -1.583333       NaN   \n",
       "2732  hirid    11135  0.112975  3.485816  3.332205 -1.650000       NaN   \n",
       "\n",
       "         urine        mg  ca  ...  year  urgency  ethnic  icu_adm_dow  \\\n",
       "3774  5.623972  0.552218 NaN  ...   NaN      NaN    None     Thursday   \n",
       "4551  5.734066  0.556503 NaN  ...   NaN      NaN    None       Monday   \n",
       "5796  5.560046       NaN NaN  ...   NaN      NaN    None       Sunday   \n",
       "3345  5.808582       NaN NaN  ...   NaN      NaN    None      Tuesday   \n",
       "6057  5.306136       NaN NaN  ...   NaN      NaN    None       Monday   \n",
       "...        ...       ...  ..  ...   ...      ...     ...          ...   \n",
       "4373  5.624400  0.377477 NaN  ...   NaN      NaN    None     Thursday   \n",
       "7891  4.917566  0.737480 NaN  ...   NaN      NaN    None       Monday   \n",
       "4859  4.216416  0.760469 NaN  ...   NaN      NaN    None       Monday   \n",
       "3264  5.084565       NaN NaN  ...   NaN      NaN    None      Tuesday   \n",
       "2732  5.345411  0.613866 NaN  ...   NaN      NaN    None       Friday   \n",
       "\n",
       "      hosp_adm_dow  adm_caregiver  adm_provider  numbedscategory  \\\n",
       "3774      Thursday           [-1]     [missing]             None   \n",
       "4551        Monday           [-1]     [missing]             None   \n",
       "5796        Sunday           [-1]     [missing]             None   \n",
       "3345       Tuesday           [-1]     [missing]             None   \n",
       "6057        Monday           [-1]     [missing]             None   \n",
       "...            ...            ...           ...              ...   \n",
       "4373      Thursday           [-1]     [missing]             None   \n",
       "7891        Monday           [-1]     [missing]             None   \n",
       "4859        Monday           [-1]     [missing]             None   \n",
       "3264       Tuesday           [-1]     [missing]             None   \n",
       "2732        Friday           [-1]     [missing]             None   \n",
       "\n",
       "      teachingstatus  region  \n",
       "3774            None    None  \n",
       "4551            None    None  \n",
       "5796            None    None  \n",
       "3345            None    None  \n",
       "6057            None    None  \n",
       "...              ...     ...  \n",
       "4373            None    None  \n",
       "7891            None    None  \n",
       "4859            None    None  \n",
       "3264            None    None  \n",
       "2732            None    None  \n",
       "\n",
       "[6781 rows x 72 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data['hirid']['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance from Training to Target Data - Parameters chosen via GridCV on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__boosting_type': ['gbdt'],\n",
       " 'model__learning_rate': [0.01, 0.1, 0.3],\n",
       " 'model__n_estimators': [100, 800],\n",
       " 'model__num_leaves': [50, 200, 1024],\n",
       " 'model__feature_fraction': [0.5, 0.9],\n",
       " 'model__verbose': [-1]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'model__' + key : value for key, value in params['lgbm'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_grid_search = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    print(name)\n",
    "    if name not in ['ols', 'anchor']:\n",
    "        search = GridSearchCV(pipe, param_grid= {'model__' + key : value for key, value in params[name].items()})\n",
    "        search.fit(_data['eicu']['train'], _data['eicu']['train']['outcome'])\n",
    "        print('finsihed GCV')\n",
    "        pipe.set_params(**search.best_params_)\n",
    "    \n",
    "    pipe.fit(_data['eicu']['train'], _data['eicu']['train']['outcome'])\n",
    "    \n",
    "    for source in sources: \n",
    "        print(source)\n",
    "        if source != 'eicu':\n",
    "            if name not in ['ols', 'anchor']:\n",
    "                mse_grid_search[name] = {'parameters': search.best_params_,\n",
    "                'MSE on {source}' : mean_squared_error(_data[source]['train']['outcome'], pipe.predict(_data[source]['train']))}\n",
    "            else: \n",
    "                mse_grid_search[name] = {'parameters': None,\n",
    "                'MSE on {source}' : mean_squared_error(_data[source]['train']['outcome'], pipe.predict(_data[source]['train']))}\n",
    "        print(f'Completed {name} run on {source}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance from Training to Target Data - Parameters chosen via Evaluation on Target Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: \n",
    "- Train Data with different parameters on Training set\n",
    "- Evaluate Train Data on fine tuning data from target set \n",
    "- choose the best performing parameters\n",
    "- do this for all possible n from the fine tuning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('gbdt', 0.01, 100, 50, 0.5, -1)\n",
      "1 ('gbdt', 0.01, 100, 50, 0.9, -1)\n",
      "2 ('gbdt', 0.01, 100, 200, 0.5, -1)\n",
      "3 ('gbdt', 0.01, 100, 200, 0.9, -1)\n",
      "4 ('gbdt', 0.01, 100, 1024, 0.5, -1)\n",
      "5 ('gbdt', 0.01, 100, 1024, 0.9, -1)\n",
      "6 ('gbdt', 0.01, 800, 50, 0.5, -1)\n",
      "7 ('gbdt', 0.01, 800, 50, 0.9, -1)\n",
      "8 ('gbdt', 0.01, 800, 200, 0.5, -1)\n",
      "9 ('gbdt', 0.01, 800, 200, 0.9, -1)\n",
      "10 ('gbdt', 0.01, 800, 1024, 0.5, -1)\n",
      "11 ('gbdt', 0.01, 800, 1024, 0.9, -1)\n",
      "12 ('gbdt', 0.1, 100, 50, 0.5, -1)\n",
      "13 ('gbdt', 0.1, 100, 50, 0.9, -1)\n",
      "14 ('gbdt', 0.1, 100, 200, 0.5, -1)\n",
      "15 ('gbdt', 0.1, 100, 200, 0.9, -1)\n",
      "16 ('gbdt', 0.1, 100, 1024, 0.5, -1)\n",
      "17 ('gbdt', 0.1, 100, 1024, 0.9, -1)\n",
      "18 ('gbdt', 0.1, 800, 50, 0.5, -1)\n",
      "19 ('gbdt', 0.1, 800, 50, 0.9, -1)\n",
      "20 ('gbdt', 0.1, 800, 200, 0.5, -1)\n",
      "21 ('gbdt', 0.1, 800, 200, 0.9, -1)\n",
      "22 ('gbdt', 0.1, 800, 1024, 0.5, -1)\n",
      "23 ('gbdt', 0.1, 800, 1024, 0.9, -1)\n",
      "24 ('gbdt', 0.3, 100, 50, 0.5, -1)\n",
      "25 ('gbdt', 0.3, 100, 50, 0.9, -1)\n",
      "26 ('gbdt', 0.3, 100, 200, 0.5, -1)\n",
      "27 ('gbdt', 0.3, 100, 200, 0.9, -1)\n",
      "28 ('gbdt', 0.3, 100, 1024, 0.5, -1)\n",
      "29 ('gbdt', 0.3, 100, 1024, 0.9, -1)\n",
      "30 ('gbdt', 0.3, 800, 50, 0.5, -1)\n",
      "31 ('gbdt', 0.3, 800, 50, 0.9, -1)\n",
      "32 ('gbdt', 0.3, 800, 200, 0.5, -1)\n",
      "33 ('gbdt', 0.3, 800, 200, 0.9, -1)\n",
      "34 ('gbdt', 0.3, 800, 1024, 0.5, -1)\n",
      "35 ('gbdt', 0.3, 800, 1024, 0.9, -1)\n"
     ]
    }
   ],
   "source": [
    "for comb, param_set in enumerate(itertools.product(*params['lgbm'].values())):\n",
    "    print(comb, param_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm\n",
      "finished 0 on source eicu\n",
      "finished 0 on source hirid\n",
      "finished 0 on source mimic\n",
      "finished 0 on source miiv\n",
      "finished 1 on source eicu\n",
      "finished 1 on source hirid\n",
      "finished 1 on source mimic\n",
      "finished 1 on source miiv\n",
      "finished 2 on source eicu\n",
      "finished 2 on source hirid\n",
      "finished 2 on source mimic\n",
      "finished 2 on source miiv\n",
      "finished 3 on source eicu\n",
      "finished 3 on source hirid\n",
      "finished 3 on source mimic\n",
      "finished 3 on source miiv\n",
      "finished 4 on source eicu\n",
      "finished 4 on source hirid\n",
      "finished 4 on source mimic\n",
      "finished 4 on source miiv\n",
      "finished 5 on source eicu\n",
      "finished 5 on source hirid\n",
      "finished 5 on source mimic\n",
      "finished 5 on source miiv\n",
      "finished 6 on source eicu\n",
      "finished 6 on source hirid\n",
      "finished 6 on source mimic\n",
      "finished 6 on source miiv\n",
      "finished 7 on source eicu\n",
      "finished 7 on source hirid\n",
      "finished 7 on source mimic\n",
      "finished 7 on source miiv\n",
      "finished 8 on source eicu\n",
      "finished 8 on source hirid\n",
      "finished 8 on source mimic\n",
      "finished 8 on source miiv\n",
      "finished 9 on source eicu\n",
      "finished 9 on source hirid\n",
      "finished 9 on source mimic\n",
      "finished 9 on source miiv\n",
      "finished 10 on source eicu\n",
      "finished 10 on source hirid\n",
      "finished 10 on source mimic\n",
      "finished 10 on source miiv\n",
      "finished 11 on source eicu\n",
      "finished 11 on source hirid\n",
      "finished 11 on source mimic\n",
      "finished 11 on source miiv\n",
      "finished 12 on source eicu\n",
      "finished 12 on source hirid\n",
      "finished 12 on source mimic\n",
      "finished 12 on source miiv\n",
      "finished 13 on source eicu\n",
      "finished 13 on source hirid\n",
      "finished 13 on source mimic\n",
      "finished 13 on source miiv\n",
      "finished 14 on source eicu\n",
      "finished 14 on source hirid\n",
      "finished 14 on source mimic\n",
      "finished 14 on source miiv\n",
      "finished 15 on source eicu\n",
      "finished 15 on source hirid\n",
      "finished 15 on source mimic\n",
      "finished 15 on source miiv\n",
      "finished 16 on source eicu\n",
      "finished 16 on source hirid\n",
      "finished 16 on source mimic\n",
      "finished 16 on source miiv\n",
      "finished 17 on source eicu\n",
      "finished 17 on source hirid\n",
      "finished 17 on source mimic\n",
      "finished 17 on source miiv\n",
      "finished 18 on source eicu\n",
      "finished 18 on source hirid\n",
      "finished 18 on source mimic\n",
      "finished 18 on source miiv\n",
      "finished 19 on source eicu\n",
      "finished 19 on source hirid\n",
      "finished 19 on source mimic\n",
      "finished 19 on source miiv\n",
      "finished 20 on source eicu\n",
      "finished 20 on source hirid\n",
      "finished 20 on source mimic\n",
      "finished 20 on source miiv\n",
      "finished 21 on source eicu\n",
      "finished 21 on source hirid\n",
      "finished 21 on source mimic\n",
      "finished 21 on source miiv\n",
      "finished 22 on source eicu\n",
      "finished 22 on source hirid\n",
      "finished 22 on source mimic\n",
      "finished 22 on source miiv\n",
      "finished 23 on source eicu\n",
      "finished 23 on source hirid\n",
      "finished 23 on source mimic\n",
      "finished 23 on source miiv\n",
      "finished 24 on source eicu\n",
      "finished 24 on source hirid\n",
      "finished 24 on source mimic\n",
      "finished 24 on source miiv\n",
      "finished 25 on source eicu\n",
      "finished 25 on source hirid\n",
      "finished 25 on source mimic\n",
      "finished 25 on source miiv\n",
      "finished 26 on source eicu\n",
      "finished 26 on source hirid\n",
      "finished 26 on source mimic\n",
      "finished 26 on source miiv\n",
      "finished 27 on source eicu\n",
      "finished 27 on source hirid\n",
      "finished 27 on source mimic\n",
      "finished 27 on source miiv\n",
      "finished 28 on source eicu\n",
      "finished 28 on source hirid\n",
      "finished 28 on source mimic\n",
      "finished 28 on source miiv\n",
      "finished 29 on source eicu\n",
      "finished 29 on source hirid\n",
      "finished 29 on source mimic\n",
      "finished 29 on source miiv\n",
      "finished 30 on source eicu\n",
      "finished 30 on source hirid\n",
      "finished 30 on source mimic\n",
      "finished 30 on source miiv\n",
      "finished 31 on source eicu\n",
      "finished 31 on source hirid\n",
      "finished 31 on source mimic\n",
      "finished 31 on source miiv\n",
      "finished 32 on source eicu\n",
      "finished 32 on source hirid\n",
      "finished 32 on source mimic\n",
      "finished 32 on source miiv\n",
      "finished 33 on source eicu\n",
      "finished 33 on source hirid\n",
      "finished 33 on source mimic\n",
      "finished 33 on source miiv\n",
      "finished 34 on source eicu\n",
      "finished 34 on source hirid\n",
      "finished 34 on source mimic\n",
      "finished 34 on source miiv\n",
      "finished 35 on source eicu\n",
      "finished 35 on source hirid\n",
      "finished 35 on source mimic\n",
      "finished 35 on source miiv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clear_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                 results[name][comb][source][n] \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: para,\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mMSE on Eval. Set from Target\u001b[39m\u001b[39m'\u001b[39m: mse_eval,\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mMSE on Target\u001b[39m\u001b[39m'\u001b[39m: mse_test\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                 }\n\u001b[1;32m     <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfinished \u001b[39m\u001b[39m{\u001b[39;00mcomb\u001b[39m}\u001b[39;00m\u001b[39m on source \u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/u/lucabri/Schreibtisch/MasterThesis/Experiments.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m clear_output()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clear_output' is not defined"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    if name not in ['ols']:\n",
    "        results[name] = {}\n",
    "        print(name)\n",
    "        for comb, param_set in enumerate(itertools.product(*params[name].values())):\n",
    "            para = dict(zip(params[name].keys(), param_set))\n",
    "            pipe.named_steps['model'].set_params(**para)\n",
    "            pipe.fit(_data['eicu']['train'], _data['eicu']['train']['outcome'])\n",
    "            results[name][comb] = {}\n",
    "            for source in sources: \n",
    "                results[name][comb][source] = {}\n",
    "                if source != 'eicu':\n",
    "                    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "                        y_pred_eval = pipe.predict(_data[source]['test'].head(n))\n",
    "                        y_pred_test = pipe.predict(_data[source]['train'])\n",
    "                        \n",
    "                        mse_eval = mean_squared_error(_data[source]['test']['outcome'].head(n), y_pred_eval)\n",
    "                        mse_test = mean_squared_error(_data[source]['train']['outcome'], y_pred_test)\n",
    "                        \n",
    "                        results[name][comb][source][n] = {\n",
    "                            'params': para,\n",
    "                            'MSE on Eval. Set from Target': mse_eval,\n",
    "                            'MSE on Target': mse_test\n",
    "                        }\n",
    "                print(f'finished {comb} on source {source}')\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(X_train, y_train, X_test, y_test, p, results):\n",
    "    mse_for_n = []\n",
    "    i = 0\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        p.named_steps['model'].set_params(**results[i]['best_params'])\n",
    "        p.fit(X_train, y_train)\n",
    "        y_pred = p.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_for_n.append({'n': n, 'mse': mse})\n",
    "        i += 1\n",
    "    return mse_for_n\n",
    "\n",
    "mse_eicu_to_hirid_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p1, results_p1_hirid)\n",
    "mse_eicu_to_hirid_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p2, results_p2_hirid)\n",
    "mse_eicu_to_hirid_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p4, results_p4_hirid)\n",
    "\n",
    "mse = {}\n",
    "\n",
    "for source in sources: \n",
    "    for name, pipe in pipelines.items():\n",
    "        \n",
    "        for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "            p.named_steps['model'].set_params(**results[i]['best_params'])\n",
    "            p.fit(X_train, y_train)\n",
    "            y_pred = p.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse_for_n.append({'n': n, 'mse': mse})\n",
    "\n",
    "    mse[source] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_parameters(Xy_train, Xy_tuning_data, p, param_grid):\n",
    "    results_for_n = []\n",
    "\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        best_params = None\n",
    "        best_mse = float('inf')  # Initialize with a large value\n",
    "\n",
    "        # Iterate over all possible combinations of hyperparameters\n",
    "        for param_set in itertools.product(*param_grid.values()):\n",
    "            params = dict(zip(param_grid.keys(), param_set))\n",
    "        \n",
    "            p.named_steps['model'].set_params(**params)\n",
    "            p.fit(Xy_train, Xy_train['outcome'])\n",
    "            y_pred = p.predict(Xy_tuning_data.head(n))\n",
    "            mse = mean_squared_error(Xy_tuning_data['outcome'].head(n), y_pred)\n",
    "\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_params = params\n",
    "\n",
    "        results_for_n.append({'n': n, 'best_params': best_params, 'best_mse': best_mse})\n",
    "\n",
    "    return results_for_n\n",
    "\n",
    "results_p1_hirid = find_best_parameters(Xy_train, Xy_tuning_hirid, p1, param_grid_lgbm)\n",
    "results_p2_hirid = find_best_parameters(Xy_train, Xy_tuning_hirid, p2, param_grid_rf)\n",
    "results_p4_hirid = find_best_parameters(Xy_train, Xy_tuning_hirid, p4, param_grid_anchor)\n",
    "\n",
    "results_p1_mimic = find_best_parameters(Xy_train, Xy_tuning_mimic, p1, param_grid_lgbm)\n",
    "results_p2_mimic = find_best_parameters(Xy_train, Xy_tuning_mimic, p2, param_grid_rf)\n",
    "results_p4_mimic = find_best_parameters(Xy_train, Xy_tuning_mimic, p4, param_grid_anchor)\n",
    "\n",
    "results_p1_miiv = find_best_parameters(Xy_train, Xy_tuning_miiv, p1, param_grid_lgbm)\n",
    "results_p2_miiv = find_best_parameters(Xy_train, Xy_tuning_miiv, p2, param_grid_rf)\n",
    "results_p4_miiv = find_best_parameters(Xy_train, Xy_tuning_miiv, p4, param_grid_anchor)\n",
    "\n",
    "def calculate_mse(X_train, y_train, X_test, y_test, p, results):\n",
    "    mse_for_n = []\n",
    "    i = 0\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        p.named_steps['model'].set_params(**results[i]['best_params'])\n",
    "        p.fit(X_train, y_train)\n",
    "        y_pred = p.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_for_n.append({'n': n, 'mse': mse})\n",
    "        i += 1\n",
    "    return mse_for_n\n",
    "\n",
    "mse_eicu_to_hirid_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p1, results_p1_hirid)\n",
    "mse_eicu_to_hirid_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p2, results_p2_hirid)\n",
    "mse_eicu_to_hirid_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p4, results_p4_hirid)\n",
    "\n",
    "mse_eicu_to_mimic_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p1, results_p1_mimic)\n",
    "mse_eicu_to_mimic_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p2, results_p2_mimic)\n",
    "mse_eicu_to_mimic_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p4, results_p4_mimic)\n",
    "\n",
    "mse_eicu_to_miiv_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_miiv, Xy_test_miiv['outcome'], p1, results_p1_miiv)\n",
    "mse_eicu_to_miiv_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_miiv, Xy_test_miiv['outcome'], p2, results_p2_miiv)\n",
    "mse_eicu_to_miiv_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_miiv, Xy_test_miiv['outcome'], p4, results_p4_miiv)\n",
    "\n",
    "# OLS MSE Calculation\n",
    "p3.fit(Xy_train, Xy_train['outcome'])\n",
    "mse_eicu_to_hirid_p3 = mean_squared_error(Xy_test_hirid['outcome'], p3.predict(Xy_test_hirid))\n",
    "mse_eicu_to_hirid_dummy_prediction = mean_squared_error(Xy_test_hirid['outcome'], np.full_like(Xy_test_hirid['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "mse_eicu_to_mimic_p3 = mean_squared_error(Xy_test_mimic['outcome'], p3.predict(Xy_test_mimic))\n",
    "mse_eicu_to_mimic_dummy_prediction = mean_squared_error(Xy_test_mimic['outcome'], np.full_like(Xy_test_mimic['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "mse_eicu_to_miiv_p3 = mean_squared_error(Xy_test_miiv['outcome'], p3.predict(Xy_test_miiv))\n",
    "mse_eicu_to_miiv_dummy_prediction = mean_squared_error(Xy_test_miiv['outcome'], np.full_like(Xy_test_miiv['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "def plotting(mse_p1, mse_p2, mse_p3, mse_p4, mse_baseline1, mse_baseline2, train, target):\n",
    "    n = [25, 50, 100, 200, 400, 800, 1600]\n",
    "    plt.plot(n, ([item['mse'] for item in mse_p1]), marker='o', linestyle='-', label = 'LGBM')\n",
    "    plt.plot(n, [item['mse'] for item in mse_p2], marker='o', linestyle='-', label = 'RF')\n",
    "    plt.plot(n, [item['mse'] for item in mse_p4], marker='o', linestyle='-', label = 'Anchor')\n",
    "    plt.axhline(y=mse_p3, color='black', linestyle='-', label='OLS Baseline')\n",
    "    plt.axhline(y=mse_baseline1, color='green', linestyle='-', label='LGBM Baseline')\n",
    "    plt.axhline(y=mse_baseline2, color='purple', linestyle='-', label='RF Baseline')\n",
    "    #plt.axhline(y = mean_squared_error(Xy_test_new['outcome'], np.full_like(Xy_test_new['outcome'],Xy_train[outcome].mean())), color = 'black', label='Train Average')\n",
    "    plt.title(f'Parameters for Model chosen with evaluation on n Data Points from Target Distribution {target}')\n",
    "    plt.xlabel('Number of Data Points (n)')\n",
    "    plt.ylabel('Mean Squared Error (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plotting(mse_eicu_to_hirid_p1, mse_eicu_to_hirid_p2, mse_eicu_to_hirid_p3, mse_eicu_to_hirid_p4, mse_grid_lgbm_hirid, mse_grid_rf_hirid, 'Eicu', 'Hirid')\n",
    "plotting(mse_eicu_to_mimic_p1, mse_eicu_to_mimic_p2, mse_eicu_to_mimic_p3, mse_eicu_to_mimic_p4, mse_grid_lgbm_mimic, mse_grid_rf_mimic, 'Eicu', 'Mimic')\n",
    "plotting(mse_eicu_to_miiv_p1, mse_eicu_to_miiv_p2, mse_eicu_to_miiv_p3, mse_eicu_to_miiv_p4, mse_grid_lgbm_miiv, mse_grid_rf_miiv, 'Eicu', 'Miiv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Woche5/parameters_dict.pkl\n",
      "MSE values saved to Woche5/mse_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "mse_dict = {}\n",
    "\n",
    "results_dict['p1_baseline'] = grid_search_params_lgbm\n",
    "results_dict['p2_baseline'] = grid_search_params_rf\n",
    "results_dict['p1_hirid'] = results_p1_hirid\n",
    "results_dict['p2_hirid'] = results_p2_hirid\n",
    "results_dict['p4_hirid'] = results_p4_hirid\n",
    "\n",
    "results_dict['p1_mimic'] = results_p1_mimic\n",
    "results_dict['p2_mimic'] = results_p2_mimic\n",
    "results_dict['p4_mimic'] = results_p4_mimic\n",
    "\n",
    "results_dict['p1_miiv'] = results_p1_miiv\n",
    "results_dict['p2_miiv'] = results_p2_miiv\n",
    "results_dict['p4_miiv'] = results_p4_miiv\n",
    "\n",
    "mse_dict['eicu_to_hirid_p1_baseline'] = mse_grid_lgbm_hirid\n",
    "mse_dict['eicu_to_hirid_p1'] = mse_eicu_to_hirid_p1\n",
    "mse_dict['eicu_to_hirid_p2'] = mse_eicu_to_hirid_p2\n",
    "mse_dict['eicu_to_hirid_p3'] = mse_eicu_to_hirid_p3\n",
    "mse_dict['eicu_to_hirid_p4'] = mse_eicu_to_hirid_p4\n",
    "\n",
    "mse_dict['eicu_to_mimic_p1_baseline'] = mse_grid_lgbm_mimic\n",
    "mse_dict['eicu_to_mimic_p1'] = mse_eicu_to_mimic_p1\n",
    "mse_dict['eicu_to_mimic_p2'] = mse_eicu_to_mimic_p2\n",
    "mse_dict['eicu_to_mimic_p3'] = mse_eicu_to_mimic_p3\n",
    "mse_dict['eicu_to_mimic_p4'] = mse_eicu_to_mimic_p4\n",
    "\n",
    "mse_dict['eicu_to_miiv_p1_baseline'] = mse_grid_lgbm_miiv\n",
    "mse_dict['eicu_to_miiv_p1'] = mse_eicu_to_miiv_p1\n",
    "mse_dict['eicu_to_miiv_p2'] = mse_eicu_to_miiv_p2\n",
    "mse_dict['eicu_to_miiv_p3'] = mse_eicu_to_miiv_p3\n",
    "mse_dict['eicu_to_miiv_p4'] = mse_eicu_to_miiv_p4\n",
    "\n",
    "mse_dict['eicu_to_hirid_dummy_prediction'] = mse_eicu_to_hirid_dummy_prediction\n",
    "mse_dict['eicu_to_mimic_dummy_prediction'] = mse_eicu_to_mimic_dummy_prediction\n",
    "mse_dict['eicu_to_miiv_dummy_prediction'] = mse_eicu_to_miiv_dummy_prediction\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the file paths to save the dictionaries\n",
    "results_file_path = 'Woche5/parameters_dict.pkl'\n",
    "mse_file_path = 'Woche5/mse_dict.pkl'\n",
    "\n",
    "# Save the results dictionary to a file\n",
    "with open(results_file_path, 'wb') as results_file:\n",
    "    pickle.dump(results_dict, results_file)\n",
    "\n",
    "# Save the MSE dictionary to a file\n",
    "with open(mse_file_path, 'wb') as mse_file:\n",
    "    pickle.dump(mse_dict, mse_file)\n",
    "\n",
    "# Optionally, you can print a message to confirm the saving process\n",
    "print(f\"Results saved to {results_file_path}\")\n",
    "print(f\"MSE values saved to {mse_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedAnchor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, anchor_params=None, lgbm_params=None):\n",
    "        # Initialize parameters\n",
    "        self.anchor_params = anchor_params if anchor_params is not None else {}\n",
    "        self.lgbm_params = lgbm_params if lgbm_params is not None else {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize and fit the Anchor Regression model\n",
    "        self.anchor_model = AnchorRegression(**self.anchor_params)\n",
    "        self.anchor_model.fit(X, y)\n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = y - self.anchor_model.predict(X)\n",
    "\n",
    "        # Initialize and fit the LGBMRegressor with residuals\n",
    "        self.lgbm_model = LGBMRegressor(**self.lgbm_params)\n",
    "        self.lgbm_model.fit(X, residuals)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if fit has been called\n",
    "        if not hasattr(self, 'anchor_model') or not hasattr(self, 'lgbm_model'):\n",
    "            raise AttributeError(\"Models have not been fitted. Call fit() first.\")\n",
    "\n",
    "        # Make predictions\n",
    "        anchor_predictions = self.anchor_model.predict(X)\n",
    "        lgbm_predictions = self.lgbm_model.predict(X)\n",
    "\n",
    "        # Combine predictions\n",
    "        return anchor_predictions + lgbm_predictions\n",
    "    \n",
    "p5 = Pipeline(steps=[\n",
    "    ('preprocessing', anchor_preprocessor),\n",
    "    ('model', CustomizedAnchor())\n",
    "])\n",
    "\n",
    "\n",
    "def find_custom_parameters(Xy_train, Xy_tuning_data, p, param1_grid, param2_grid):\n",
    "    results_for_n = []\n",
    "\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        best_params1 = None\n",
    "        best_params2 = None\n",
    "        best_mse = float('inf') \n",
    "\n",
    "        param1_combinations = list(itertools.product(*param1_grid.values()))\n",
    "        param1_combinations_bar = tqdm(param1_combinations, desc=f\"n = {n}\")\n",
    "\n",
    "        for param1_set in itertools.product(*param1_grid.values()):\n",
    "            param1 = dict(zip(param1_grid.keys(), param1_set))\n",
    "\n",
    "            for param2_set in itertools.product(*param2_grid.values()):\n",
    "                param2 = dict(zip(param2_grid.keys(), param2_set))\n",
    "                \n",
    "\n",
    "                p.named_steps['model'].set_params(anchor_params= param1, lgbm_params=param2)\n",
    "                p.fit(Xy_train, Xy_train['outcome'])\n",
    "                y_pred = p.predict(Xy_tuning_data.head(n))\n",
    "                mse = mean_squared_error(Xy_tuning_data['outcome'].head(n), y_pred)\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_params1 = param1\n",
    "                    best_params2 = param2\n",
    "\n",
    "        results_for_n.append({'n': n, 'best_params set 1': best_params1, 'best_params set 2': best_params2, 'best_mse': best_mse})\n",
    "\n",
    "    return results_for_n\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10, 10000],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.00001, 0.001, 0.1]\n",
    "}\n",
    "\n",
    "results_p5_hirid = find_custom_parameters(Xy_train, Xy_tuning_hirid, p5, param_grid_anchor, param_grid_lgbm)\n",
    "#results_p5_mimic = find_custom_parameters(Xy_train, Xy_tuning_mimic, p5, param_grid_anchor, param_grid_lgbm)\n",
    "#results_p5_miiv = find_custom_parameters(Xy_train, Xy_tuning_miiv, p5, param_grid_anchor, param_grid_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(X_train, y_train, X_test, y_test, p, results):\n",
    "    mse_for_n = []\n",
    "    i = 0\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        p.named_steps['model'].set_params(anchor_params= results[i]['best_params set 1'], lgbm_params= results[i]['best_params set 2'])\n",
    "        p.fit(X_train, y_train)\n",
    "        y_pred = p.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_for_n.append({'n': n, 'mse': mse})\n",
    "        i += 1\n",
    "    return mse_for_n\n",
    "\n",
    "mse_eicu_to_hirid_p5 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_hirid, Xy_test_hirid['outcome'], p5, results_p5_hirid)\n",
    "#mse_eicu_to_mimic_p5 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p5, results_p5_mimic)\n",
    "#mse_eicu_to_miiv_p5 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic, Xy_test_mimic['outcome'], p5, results_p5_miiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations eICU to X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "The hyperparameters were selected to minimize the Mean Squared Error (MSE) on the fine-tuning dataset of the target distribution. This fine-tuning dataset consists of various sizes, including n = 25, 50, 100, 200, 400, 800, and 1600 data points from the target distribution. Initially, we randomly selected 1600 data points from the target data and named it Xy_tuning_data, which is distinct from the final evaluation dataset used to generate the plotted MSE after model training, called Xy_test_new. \n",
    "```\n",
    "**Evaluation Process:**\n",
    "```markdown\n",
    "Our evaluation process follows these steps:\n",
    "\n",
    "1. For each combination of the parameters, we train the model on the training data.\n",
    "2. Next, we calculate the MSE on the fine-tuning data from the training distribution.\n",
    "3. For each n value, we select the parameter combination that minimizes the MSE on the fine-tuning data.\n",
    "\n",
    "We have four distinct pipelines for our models:\n",
    "\n",
    "- LGBM pipeline: p1\n",
    "- Random Forest pipeline: p2\n",
    "- OLS pipeline: p3\n",
    "- Anchor pipeline: p4\n",
    "\n",
    "For OLS, we follow a slightly different approach. We train the model on the training data and evaluate it directly on the target data.\n",
    "\n",
    "In a subsequent step, we repeat the parameter selection process on the training data and calculate the MSE on the target data - we call this approach the Baseline. The plot displays the model's performance along with the Baseline.\n",
    "```\n",
    "\n",
    "**Model Performance:**\n",
    "```markdown\n",
    "Interestingly, none of the models managed to substentially outperform the Baselines on any dataset. \n",
    "\n",
    "eICU --> Hirid:\n",
    "- p1/LGBM: \n",
    "    - In an overall trend, LGBM is able improve its precision with increasing n, however, it is still not able to get the same precision as the Baseline\n",
    "    - Similar to the RF, it restricts the number of leafs drastically\n",
    "- p2/RF: \n",
    "    - The same parameters have been chosen every time\n",
    "    - Its parameters coincide with those choosen by GridCV for n < 1600 \n",
    "    - Its performance decreses when allowing 1600 fine-tuning datapoints, aka as soon as the distr. shift becomes noticable\n",
    "    - Interestengly, it choses a small number of leaves compared to the size of the available fine-tuning dataset\n",
    "- p4/Anchor and p3/OLS:\n",
    "    - Surprisingly, Anchor fails to identify a significant distributional shift, aka. it chooses consistently gamma = 1, i.e. it coincides with OLS\n",
    "    - Unsurprisingly, it performs almost identical to OLS, only the regularization influences the performance\n",
    "    - The more fine-tuning data we allow, the less regularization it choses\n",
    "    - Not able to beat the LGBM Baseline\n",
    "\n",
    "eICU --> Mimic:\n",
    "- p1/LGBM: \n",
    "    - In an overall trend, LGBM is able improve its precision with increasing n, and it is able to consistently beat its Baseline\n",
    "    - It is able to improve its performance significantly by restricting itself to a small number of leafs\n",
    "- p2/RF: \n",
    "    - The same parameters have been chosen for every n \n",
    "    - Its parameters coincide with those choosen by GridCV and the performance too\n",
    "    - It does not seem to notice a distr. shift\n",
    "    - Interestengly, it choses a small number of leaves compared to the size of the available fine-tuning dataset\n",
    "- p4/Anchor and p3/OLS:\n",
    "    - Surprisingly, Anchor fails to identify a significant distributional shift, aka. it chooses consistently gamma = 1, i.e. it coincides with OLS\n",
    "    - Best performing model \n",
    "\n",
    "eICU --> Miiv:\n",
    "- p1/LGBM: \n",
    "    - In an overall trend, LGBM is able improve its precision with increasing n, but not able to consistently beat its Baseline\n",
    "    - It is able to improve its performance significantly by restricting itself to a small number of leafs\n",
    "- p2/RF: \n",
    "    - It is able to adapt itself to the baseline parameters and coincides most of the time with the Baseline\n",
    "    - It does not seem to notice a distr. shift\n",
    "    - Interestengly, it choses a small number of leaves compared to the size of the available fine-tuning dataset\n",
    "- p4/Anchor and p3/OLS:\n",
    "    - Surprisingly, Anchor a distributional shift in the beginning, but fails to identify it consistently\n",
    "    - However, Anchor is able to outperform OLS by a margin, most likely due to the regularization / OLS seems to have highly correlated features that destroy its prediction\n",
    "    - Not able to beat the LGBM Baseline\n",
    "\n",
    "The evaluation mse on the fine-tuning data when performing parameter selection does not seem to have any predictive power of the outcome of the mse on the target data\n",
    "\n",
    "This observation could be attributed to the limited available hyperparameters. It would be intriguing to investigate whether the models can surpass their Baseline when provided with more possibilities. A potential follow-up question is whether predictive performance improves with n=2000 (Hypothesis: Yes, as the prediction benefits from more accurate data).\n",
    "\n",
    "Remarkably, all models outperformed the average prediction of the training data by a substantial margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "The hyperparameters were chosen from three distinct parameter grids:\n",
    "```\n",
    "\n",
    "**LightGBM (param_grid_lgbm):**\n",
    "```python\n",
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 200, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "```\n",
    "\n",
    "**RF (param_grid_rf):**\n",
    "```python\n",
    "param_grid_rf = {\n",
    "    'boosting_type': ['rf'],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 800], \n",
    "    'num_leaves': [50, 200, 1024], \n",
    "    'feature_fraction': [0.5, 0.9]\n",
    "}\n",
    "```\n",
    "\n",
    "**Anchor (param_grid_anchor):**\n",
    "```python\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10, 10000],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.00001, 0.001, 0.1]\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom Anchor:**\n",
    "```python\n",
    "param_grid_lgbm = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'learning_rate': [0.01, 0.3], # Gradient learning rate\n",
    "    'n_estimators': [100, 800], # number of boosting iterations\n",
    "    'num_leaves': [50, 1024], # Control tree structure - max. number of leaves in tree (num_leaves < 2^max depth)\n",
    "    'feature_fraction': [0.5, 0.9] # % of features to sample when training each tree\n",
    "}\n",
    "param_grid_anchor = {\n",
    "    'gamma': [1, 10],\n",
    "    'instrument_regex': ['anchor'],\n",
    "    'alpha': [0.001, 0.1]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for both, LGBM and RF the algorithm chooses the same set of parameters when evaluated on the tuning set from the target date every time. Hence unsurprisingly, the mse is constant. Similar for Anchor, we choose alomst every time the same set of parameters and do not improve the mse.\n",
    "\n",
    "For LGBM: The set of parameters chosen by grid search outperforms the parameters chosen by evaluation on the target. \n",
    "\n",
    "For RF: The set of parameters is the same, i.e. same performance \n",
    "\n",
    "For Anchor: No CV on train\n",
    "\n",
    "However, Anchor is again able to beat the predictive performance from OLS with the available parameters. \n",
    "\n",
    "For CustomAnchor (Anchor + LGBM Boosting): The set of parameters improves when increasing the evaluation data from the target. This method outperforms all other methods when having 1600 fine-tuning data points available.\n",
    "\n",
    "We conclude by noting that it is of utmost importance to include hyperparameters that prevent overfitting of the tree methods (compare results from parameterset 1 and parameterset 2) and are curious if the performance of CustomAnchor can be improved too when allowing these kind of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Mimic without children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test_mimic_no_children, Xy_tuning_mimic_no_children = Xy_test_mimic[Xy_test_mimic['age'] > 18], Xy_tuning_mimic[Xy_tuning_mimic['age'] > 18]\n",
    "\n",
    "mse_grid_lgbm_mimic_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], p1.predict(Xy_test_mimic_no_children))\n",
    "mse_grid_rf_mimic_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], p2.predict(Xy_test_mimic_no_children))\n",
    "\n",
    "results_p1_mimic_nC = find_best_parameters(Xy_train, Xy_tuning_mimic_no_children, p1, param_grid_lgbm)\n",
    "results_p2_mimic_nC = find_best_parameters(Xy_train, Xy_tuning_mimic_no_children, p2, param_grid_rf)\n",
    "results_p4_mimic_nC = find_best_parameters(Xy_train, Xy_tuning_mimic_no_children, p4, param_grid_anchor)\n",
    "\n",
    "mse_eicu_to_mimic_p1_nC = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic_no_children, Xy_test_mimic_no_children['outcome'], p1, results_p1_mimic_nC)\n",
    "mse_eicu_to_mimic_p2_nC = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic_no_children, Xy_test_mimic_no_children['outcome'], p2, results_p2_mimic_nC)\n",
    "mse_eicu_to_mimic_p4_nC = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_mimic_no_children, Xy_test_mimic_no_children['outcome'], p4, results_p4_mimic_nC)\n",
    "\n",
    "mse_eicu_to_mimic_p3_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], p3.predict(Xy_test_mimic_no_children))\n",
    "mse_eicu_to_mimic_dummy_prediction_nC = mean_squared_error(Xy_test_mimic_no_children['outcome'], np.full_like(Xy_test_mimic_no_children['outcome'],Xy_train[outcome].mean()))\n",
    "\n",
    "plotting(mse_eicu_to_mimic_p1_nC, mse_eicu_to_mimic_p2_nC, mse_eicu_to_mimic_p3_nC, mse_eicu_to_mimic_p4_nC, mse_grid_lgbm_mimic_nC, mse_grid_rf_mimic_nC, 'Eicu', 'Mimic no Children')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Malte Fragen beantworten: \n",
    "    - peak: Hab mit falschen Parametern getestet\n",
    "    - Sowohl RF wie LGBM mit tuning auf target distr. data sind schlechter (nie besser) als die Baselines, egal wie gross â€œnâ€ ist. Wieso?\n",
    "    - Dein MSE der OLS baseline eICU -> MIMIC III ist signifikant besser als das was ich in dem pdf das ich dir mal geschickt hatte habe (~175). Was machst du anders? ################## das ist eICU -> Hirid\n",
    "- Refit implementieren und anschauen\n",
    "- Euler\n",
    "- Connect to ADA and run jobs - muss ich dann icu & iv neu installieren --- wie?\n",
    "- ML Flow lernen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation auf hold out test set von trainingsdaten fÃ¼r lgbm vs ols \n",
    "- Preprocessing LGBM anpassen - lgbm kann mit categorical umgehen \n",
    "- andere rs fÃ¼r rf auf hirid\n",
    "- Mimic3: optimale Gammas testen\n",
    "- EV XtX und l2 reg. fÃ¼r OLS\n",
    "- Python skript lernen\n",
    "- Umgekehrt denken: erst organisieren, dann implementieren ---> Reusability im Kopf haben \n",
    "- Git clone auf Euler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
