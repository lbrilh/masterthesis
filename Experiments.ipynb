{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icu_experiments.load_data import load_data_for_prediction\n",
    "from icu_experiments.preprocessing import make_feature_preprocessing, make_anchor_preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor, Booster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from ivmodels import AnchorRegression\n",
    "\n",
    "outcome = \"hr\"\n",
    "\n",
    "Xy = load_data_for_prediction([\"eicu\"], outcome=outcome, log_transform=True)\n",
    "Xy_train = Xy['eicu']['train']\n",
    "Xy_test = Xy['eicu']['test']\n",
    "\n",
    "Xy_new = load_data_for_prediction(['hirid'], outcome=outcome, log_transform=True)\n",
    "Xy_test_new = Xy_new['hirid']['train']\n",
    "Xy_tuning_data = Xy_new['hirid']['test']\n",
    "\n",
    "preprocessing_steps = make_feature_preprocessing(missing_indicator=True)\n",
    "preprocessor = ColumnTransformer(transformers=preprocessing_steps).set_output(transform=\"pandas\") # Allow to preprocess subbsets of data differently\n",
    "\n",
    "anchor_columns = ['hospital_id']\n",
    "anchor_preprocessing_steps = make_anchor_preprocessing(anchor_columns)\n",
    "anchor_preprocessor = ColumnTransformer(\n",
    "        anchor_preprocessing_steps + preprocessing_steps\n",
    "        #preprocessing_steps\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "p1 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "p2 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LGBMRegressor())\n",
    "])\n",
    "p3 = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "p4 = Pipeline(steps=[\n",
    "    ('preprocessing', anchor_preprocessor),\n",
    "    ('model', AnchorRegression())\n",
    "])\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'model__boosting_type': ['gbdt'],  # Set the boosting type for LightGBM\n",
    "    'model__num_leaves': [15, 31],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'model__n_estimators': [50, 100, 800]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'model__boosting_type': ['rf'],  # Set the boosting type for LightGBM\n",
    "    'model__num_leaves': [15, 31],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'model__n_estimators': [50, 100, 800]\n",
    "}\n",
    "param_grid_anchor = {\n",
    "    'instrument_regex': [\"anchor\"],\n",
    "    'gamma': [1, 3.16, 10, 31.6, 100, 316, 1000, 3162, 10000],\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance from Training to Target Data - Parameters chosen via GridCV on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.924568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12228\n",
      "[LightGBM] [Info] Number of data points in the train set: 58145, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.928896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12230\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.913502\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12232\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.950313\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12189\n",
      "[LightGBM] [Info] Number of data points in the train set: 58146, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score 84.985997\n"
     ]
    }
   ],
   "source": [
    "search = GridSearchCV(p1, param_grid_lgbm)\n",
    "search.fit(Xy_train, Xy_train['outcome'])\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "grid_search_params_lgbm = search.best_params_\n",
    "print(search.best_params_)\n",
    "\n",
    "search = GridSearchCV(p2, param_grid_rf)\n",
    "search.fit(Xy_train, Xy_train['outcome'])\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "grid_search_params_rf = search.best_params_\n",
    "print(search.best_params_)\n",
    "\n",
    "p1.set_params(**grid_search_params_lgbm)\n",
    "p1.fit(Xy_train, Xy_train['outcome'])\n",
    "p2.set_params(**grid_search_params_rf)\n",
    "p2.fit(Xy_train, Xy_train['outcome'])\n",
    "\n",
    "mse_grid_lgbm = mean_squared_error(Xy_test_new['outcome'], p1.predict(Xy_test_new))\n",
    "mse_grid_rf = mean_squared_error(Xy_test_new['outcome'], p2.predict(Xy_test_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance from Training to Target Data - Parameters chosen via Evaluation on Target Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train Data with different parameters on Training set\n",
    "- Evaluate Train Data on fine tuning data from target set \n",
    "- choose the best performing parameters\n",
    "- do this for all possible n from the fine tuning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_parameters(Xy_train, Xy_tuning_data, p, param_grid):\n",
    "    # Initialize a list to store the best parameters and MSE for each n\n",
    "    results_for_n = []\n",
    "\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        # Initialize variables to keep track of the best parameters and MSE for the current n\n",
    "        best_params = None\n",
    "        best_mse = float('inf')  # Initialize with a large value\n",
    "\n",
    "        # Iterate over all possible combinations of hyperparameters\n",
    "        for param_set in itertools.product(*param_grid.values()):\n",
    "            param_dict = dict(zip(param_grid.keys(), param_set))\n",
    "\n",
    "            # Create and train the LightGBM model\n",
    "            params = {\n",
    "                **param_dict  # Include other relevant parameters\n",
    "            }\n",
    "            p.set_params(**{'model__' + key: value for key, value in params.items()})\n",
    "            p.fit(Xy_train, Xy_train['outcome'])\n",
    "\n",
    "            # Make predictions on the subset of data\n",
    "            y_pred = p.predict(Xy_tuning_data.head(n))\n",
    "\n",
    "            # Calculate mean squared error for this parameter set\n",
    "            mse = mean_squared_error(Xy_tuning_data['outcome'].head(n), y_pred)\n",
    "\n",
    "            # Check if this MSE is better than the current best for the current n\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_params = param_dict\n",
    "\n",
    "        # Store the best parameters and MSE for the current n in the list\n",
    "        results_for_n.append({'n': n, 'best_params': best_params, 'best_mse': best_mse})\n",
    "\n",
    "    return results_for_n\n",
    "\n",
    "results_p1 = find_best_parameters(Xy_train, Xy_tuning_data, p1, param_grid_lgbm)\n",
    "results_p2 = find_best_parameters(Xy_train, Xy_tuning_data, p2, param_grid_rf)\n",
    "results_p4 = find_best_parameters(Xy_train, Xy_tuning_data, p4, param_grid_anchor)\n",
    "\n",
    "def calculate_mse(X_train, y_train, X_test, y_test, p, results, boosting_type=None, is_anchor=False):\n",
    "    mse_for_n = []\n",
    "    i = 0\n",
    "    for n in [25, 50, 100, 200, 400, 800, 1600]:\n",
    "        params = {f'model__{key}': value for key, value in results[i]['best_params'].items()}\n",
    "        if not is_anchor: \n",
    "            params['model__boosting_type'] = boosting_type\n",
    "        p.set_params(**params)\n",
    "        p.fit(X_train, y_train)\n",
    "        y_pred = p.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_for_n.append({'n': n, 'mse': mse})\n",
    "        i += 1\n",
    "    return mse_for_n\n",
    "\n",
    "mse_eicu_to_hirid_p1 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_new, Xy_test_new['outcome'], p1, results_p1, 'gbdt')\n",
    "mse_eicu_to_hirid_p2 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_new, Xy_test_new['outcome'], p2, results_p2, 'rf')\n",
    "mse_eicu_to_hirid_p4 = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_new, Xy_test_new['outcome'], p4, results_p4, is_anchor=True)\n",
    "\n",
    "# OLS MSE Calculation\n",
    "p3.fit(Xy_train, Xy_train['outcome'])\n",
    "mse_eicu_to_hirid_p3 = mean_squared_error(Xy_test_new['outcome'], p3.predict(Xy_test_new, Xy_test_new['outcome']))\n",
    "mse_eicu_to_hirid_dummy_prediction = mean_squared_error(Xy_test_new['outcome'], np.full_like(Xy_test_new['outcome'],Xy_train[outcome].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [25, 50, 100, 200, 400, 800, 1600]\n",
    "plt.plot(n, ([item['mse'] for item in mse_eicu_to_hirid_p1]), marker='o', linestyle='-', label = 'LGBM')\n",
    "plt.plot(n, [item['mse'] for item in mse_eicu_to_hirid_p2], marker='o', linestyle='-', label = 'RF')\n",
    "plt.plot(n, [item['mse'] for item in mse_eicu_to_hirid_p4], marker='o', linestyle='-', label = 'Anchor')\n",
    "plt.axhline(y=mse_eicu_to_hirid_p3, color='red', linestyle='-', label='OLS Baseline')\n",
    "plt.axhline(y=mse_grid_lgbm, color='green', linestyle='-', label='LGBM Baseline')\n",
    "plt.axhline(y=mse_grid_rf, color='purple', linestyle='-', label='RF Baseline')\n",
    "#plt.axhline(y = mean_squared_error(Xy_test_new['outcome'], np.full_like(Xy_test_new['outcome'],Xy_train[outcome].mean())), color = 'black', label='Train Average')\n",
    "plt.title('Parameters for Model chosen with evaluation on n Data Points from Target Distribution')\n",
    "plt.xlabel('Number of Data Points (n)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters were chosen such that they minimize the the mse on the fine-tuning data set of the target distribution. We have n = 25, 50, 100, 200, 400, 800 and 1600 datapoints from the target distribution. In an inital step, we chose randomly 1600 data points from the target data and called it Xy_tuning_data - they are not part of the final evaulation data (i.e. the plotted mse after training). The hyperparameters were chosen from: \n",
    "\n",
    "param_grid_lgbm = { \\\n",
    "    'boosting_type': ['gbdt'], \\\n",
    "    'num_leaves': [15, 31], \\\n",
    "    'subsample': [0.8, 1.0], \\ \n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],  # Increase the range of learning rates \\\n",
    "    'n_estimators': [50, 100, 800]  # Increase the range of estimators\\\n",
    "} \\\n",
    "\n",
    "param_grid_rf = {\\\n",
    "    'boosting_type': ['rf'],\\\n",
    "    'num_leaves': [15, 31],\\\n",
    "    'subsample': [0.8, 1.0],\\\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],  # Increase the range of learning rates\\\n",
    "    'n_estimators': [50, 100, 800]  # Increase the range of estimators\\\n",
    "}\\\n",
    "\n",
    "param_grid_anchor = {\\\n",
    "    'instrument_regex': [\"anchor\"],\\\n",
    "    'gamma': [1, 3.16, 10, 31.6, 100, 316, 1000, 3162, 10000],\\\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1]\\\n",
    "}\n",
    "\n",
    "\n",
    "The parameters for evaluation were chosen as follows:\n",
    "1. For each combination of the parameters train the model on the training data\n",
    "2. Calculate the mse on the fine-tuning data from the trainings distribution \n",
    "3. For each n, choose the parameter combination such that it minimizes the mse on the fine-tuning data\n",
    "\n",
    "The LGBM-pipeline is called p1, the RF-pipeline is called p2, the OLS-pipeline is called p3, the Anchor-pipeline is called p4.\n",
    "\n",
    "For OLS we proceed different. Here we simply train the model on the trainings data and evaluate it on the target data. \n",
    "\n",
    "In a successive step, we perform the same parameter selection process but on the trainings data and calculate the mse on the target data. The performance is denoted in the plot by the model + Baseline. Surprisingly, none of the models was able to beat the Baselines. Only LGBM was able for n = 1600 to accomplish identical precision as its baseline. It seems like LGBM can already identify the same important hyperparameters when only having access to 1600 tuning data points. Already after just having 50 data points available, anchor is able to beat OLS. The predictive performance of the RF seems to be independent of the choosen parameters. The choosen parameters seem to have high influence on the predictive performance of LGBM. After only having 200 data points available, LGBM is able to reduce its mse by 8%. \n",
    "\n",
    "This may be caused by the lack of available hyperparameters. It would be interisting to see if the models are able to beat its baseline when allow more possibilities. A possible follow up question is if the predictive perfromance improves when n=2000 (Hypothesis: Yes, since the prediction has more accurate data)\n",
    "\n",
    "All models were able to beat the average-prediction of the trainings data by a lot (MSE = 283.59190335035487)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The parameters: \n",
    "'best_mse' is the mse that was reached with this parameter combination on the target data \n",
    "\n",
    "for p1 / LGBM: \n",
    "[{'n': 25,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.2,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 113.7853669744457},\n",
    " {'n': 50,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.2,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 151.03257816415527},\n",
    " {'n': 100,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.2,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 147.50302668110243},\n",
    " {'n': 200,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 31,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.1,\n",
    "   'n_estimators': 100},\n",
    "  'best_mse': 159.93871260736046},\n",
    " {'n': 400,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 31,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.1,\n",
    "   'n_estimators': 100},\n",
    "  'best_mse': 153.88405924738646},\n",
    " {'n': 800,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 31,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.1,\n",
    "   'n_estimators': 100},\n",
    "  'best_mse': 148.97829215300928},\n",
    " {'n': 1600,\n",
    "  'best_params': {'boosting_type': 'gbdt',\n",
    "   'num_leaves': 31,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.01,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 163.99952271689486}]\n",
    "\n",
    "  For p2 / RF: \n",
    "  [{'n': 25,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 140.62296079918235},\n",
    " {'n': 50,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 169.23772353266452},\n",
    " {'n': 100,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 156.54175652388255},\n",
    " {'n': 200,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 170.28568523948127},\n",
    " {'n': 400,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 159.52549273668203},\n",
    " {'n': 800,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 154.98743617760542},\n",
    " {'n': 1600,\n",
    "  'best_params': {'boosting_type': 'rf',\n",
    "   'num_leaves': 15,\n",
    "   'subsample': 0.8,\n",
    "   'learning_rate': 0.001,\n",
    "   'n_estimators': 800},\n",
    "  'best_mse': 171.13541258639327}]\n",
    "\n",
    "  For p3 / OLS: \n",
    "{'hirid': 'OLS': {'Test Error': 159.65984526272683}} \n",
    "\n",
    "For p4 / Anchor: \n",
    "[{'n': 25,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 1, 'alpha': 0.1},\n",
    "  'best_mse': 145.0284017501739},\n",
    " {'n': 50,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 1, 'alpha': 0.001},\n",
    "  'best_mse': 166.65092062151024},\n",
    " {'n': 100,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 3.16, 'alpha': 0.01},\n",
    "  'best_mse': 152.59928450652117},\n",
    " {'n': 200,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 1, 'alpha': 1e-05},\n",
    "  'best_mse': 165.26237755409892},\n",
    " {'n': 400,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 1, 'alpha': 1e-05},\n",
    "  'best_mse': 155.53044386121334},\n",
    " {'n': 800,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 1, 'alpha': 1e-05},\n",
    "  'best_mse': 151.55849601840572},\n",
    " {'n': 1600,\n",
    "  'best_params': {'instrument_regex': 'anchor', 'gamma': 1, 'alpha': 0.01},\n",
    "  'best_mse': 165.51230906082435}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation MSE\n",
    "\n",
    "For p1 / LGBM: \n",
    "[{'n': 25, 'mse': 172.77225379649616},\n",
    " {'n': 50, 'mse': 172.77225379649616},\n",
    " {'n': 100, 'mse': 172.77225379649616},\n",
    " {'n': 200, 'mse': 158.45334348900073},\n",
    " {'n': 400, 'mse': 158.45334348900073},\n",
    " {'n': 800, 'mse': 158.45334348900073},\n",
    " {'n': 1600, 'mse': 157.66217932606298}]\n",
    "\n",
    " For p2 / RF: \n",
    " [{'n': 25, 'mse': 163.39728426435462},\n",
    " {'n': 50, 'mse': 163.39728426435462},\n",
    " {'n': 100, 'mse': 163.39728426435462},\n",
    " {'n': 200, 'mse': 163.39728426435462},\n",
    " {'n': 400, 'mse': 163.39728426435462},\n",
    " {'n': 800, 'mse': 163.39728426435462},\n",
    " {'n': 1600, 'mse': 163.39728426435462}]\n",
    "\n",
    "For p4 / Anchor: \n",
    "[{'n': 25, 'mse': 159.97760862179274},\n",
    " {'n': 50, 'mse': 159.47863383193007},\n",
    " {'n': 100, 'mse': 158.96353854009493},\n",
    " {'n': 200, 'mse': 159.6254589611603},\n",
    " {'n': 400, 'mse': 159.6254589611603},\n",
    " {'n': 800, 'mse': 159.6254589611603},\n",
    " {'n': 1600, 'mse': 158.93096747576763}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline MSE\n",
    "\n",
    "Best parameter (CV score=0.430):\n",
    "{'model__boosting_type': 'gbdt', 'model__learning_rate': 0.01, 'model__n_estimators': 800, 'model__num_leaves': 31, 'model__subsample': 0.8}\n",
    "\n",
    "\n",
    "Best parameter (CV score=0.408):\n",
    "{'model__boosting_type': 'rf', 'model__learning_rate': 0.001, 'model__n_estimators': 800, 'model__num_leaves': 31, 'model__subsample': 0.8}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'model__boosting_type': ['gbdt'],  # Set the boosting type for LightGBM\n",
    "    'model__subsample': [0.6, 0.8, 1.0],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.05 0.1, 0.2, 0.3],\n",
    "    'model__n_estimators': [50, 100, 800, 3000]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'model__boosting_type': ['rf'],  # Set the boosting type for LightGBM\n",
    "    'model__subsample': [0.6, 0.8, 1.0],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'model__n_estimators': [50, 100, 800, 3000]\n",
    "}\n",
    "\n",
    "results_p1_new = find_best_parameters(Xy_train, Xy_tuning_data, p1, param_grid_lgbm)\n",
    "results_p2_new = find_best_parameters(Xy_train, Xy_tuning_data, p2, param_grid_rf)\n",
    "\n",
    "mse_eicu_to_hirid_p1_new = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_new, Xy_test_new['outcome'], p1, results_p1_new, 'gbdt')\n",
    "mse_eicu_to_hirid_p2_new = calculate_mse(Xy_train, Xy_train['outcome'], Xy_test_new, Xy_test_new['outcome'], p2, results_p2_new, 'rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, there is no predictive improvement if we allow more parameters in the selection process:\n",
    "\n",
    "For p1 / LGBM:\n",
    "[{'n': 25, 'mse': 172.77225379649616},\n",
    " {'n': 50, 'mse': 172.77225379649616},\n",
    " {'n': 100, 'mse': 172.77225379649616},\n",
    " {'n': 200, 'mse': 158.45334348900073},\n",
    " {'n': 400, 'mse': 158.45334348900073},\n",
    " {'n': 800, 'mse': 158.45334348900073},\n",
    " {'n': 1600, 'mse': 157.66217932606298}]\n",
    "\n",
    " For p2 / RF: \n",
    " [{'n': 25, 'mse': 163.39728426435462},\n",
    " {'n': 50, 'mse': 163.39728426435462},\n",
    " {'n': 100, 'mse': 163.39728426435462},\n",
    " {'n': 200, 'mse': 163.39728426435462},\n",
    " {'n': 400, 'mse': 163.39728426435462},\n",
    " {'n': 800, 'mse': 163.39728426435462},\n",
    " {'n': 1600, 'mse': 163.39728426435462}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Malte Fragen beantworten: \n",
    "    - Sowohl RF wie LGBM mit tuning auf target distr. data sind schlechter (nie besser) als die Baselines, egal wie gross n ist. Wieso?\n",
    "    - Dein MSE der OLS baseline eICU -> MIMIC III ist signifikant besser als das was ich in dem pdf das ich dir mal geschickt hatte habe (~175). Was machst du anders?\n",
    "- Andere Target Distr. anschauen\n",
    "- .md schn schreiben\n",
    "- Refit implementieren und anschauen\n",
    "- Customized Anchor implementieren"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
